{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b6b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import av\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4801a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"dataset\"\n",
    "test_dir = f'{data_dir}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701a6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_df = pd.read_csv(f'{data_dir}/test_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e81c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = {f\"{test_dir}/{k[0]}\": k[1] for k in test_label_df.values.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9096417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique label: 226\n"
     ]
    }
   ],
   "source": [
    "total_label = pd.read_csv(f'{data_dir}/ClassId.csv')\n",
    "n_classes = len(total_label['ClassId'].unique())\n",
    "print(\"total unique label:\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19280d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id_to_label = {k[0]: k[2] for k in total_label.values.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a5637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'sister', 1: 'hurry', 2: 'hungry', 3: 'enjoy_your_meal', 4: 'brother', 5: 'tree', 6: 'heavy', 7: 'cry', 8: 'family', 9: 'wise', 10: 'unwise', 11: 'kin', 12: 'shopping', 13: 'key', 14: 'mother', 15: 'friend', 16: 'ataturk', 17: 'shoe', 18: 'mirror', 19: 'same', 20: 'father', 21: 'garden', 22: 'look', 23: 'honey', 24: 'glass', 25: 'flag', 26: 'feast', 27: 'baby', 28: 'single', 29: 'wait', 30: 'I', 31: 'petrol', 32: 'together', 33: 'inform', 34: 'we', 35: 'work', 36: 'wednesday', 37: 'fork', 38: 'tea', 39: 'teapot', 40: 'hammer', 41: 'ugly', 42: 'child', 43: 'soup', 44: 'friday', 45: 'saturday', 46: 'wallet', 47: 'minute', 48: 'grandfather', 49: 'change', 50: 'topple', 51: 'government', 52: 'doctor', 53: 'full', 54: 'wedding', 55: 'yesterday', 56: 'enemy', 57: 'wall', 58: 'pharmacy', 59: 'glove', 60: 'labor', 61: 'retired', 62: 'male', 63: 'meal', 64: 'house', 65: 'yes', 66: 'married', 67: 'memorize', 68: 'elephant', 69: 'photograph', 70: 'football', 71: 'past', 72: 'get_well', 73: 'bring', 74: 'lake', 75: 'shirt', 76: 'see', 77: 'show', 78: 'laugh', 79: 'lightweight', 80: 'right', 81: 'carpet', 82: 'ill', 83: 'hospital', 84: 'fault', 85: 'towel', 86: 'no', 87: 'congratulations', 88: 'animal', 89: 'gift', 90: 'halal', 91: 'always', 92: 'never', 93: 'goodbye', 94: 'drink', 95: 'needle', 96: 'medicine', 97: 'not_interested', 98: 'light', 99: 'push', 100: 'good', 101: 'escape', 102: 'breakfast', 103: 'pencil', 104: 'radiator', 105: 'door', 106: 'sibling', 107: 'crossroads', 108: 'accident', 109: 'belt', 110: 'if_only', 111: 'who', 112: 'identity', 113: 'rent', 114: 'book', 115: 'mince', 116: 'female', 117: 'smell', 118: 'cologne', 119: 'coal', 120: 'dog', 121: 'bridge', 122: 'bad', 123: 'lap', 124: 'stain', 125: 'salary', 126: 'scissors', 127: 'tongs', 128: 'god_preserve', 129: 'angel', 130: 'be_pleased', 131: 'napkin', 132: 'stairs', 133: 'guest', 134: 'manager', 135: 'tap', 136: 'how', 137: 'why', 138: 'where', 139: 'grandmother', 140: 'oven', 141: 'room', 142: 'wood', 143: 'teacher', 144: 'school', 145: 'olympiad', 146: 'nope', 147: 'allright', 148: 'they', 149: 'forest', 150: 'fasting', 151: 'apologize', 152: 'cotton', 153: 'trousers', 154: 'money', 155: 'pastrami', 156: 'potato', 157: 'sunday', 158: 'monday', 159: 'window', 160: 'thursday', 161: 'picnic', 162: 'police', 163: 'psychology', 164: 'request', 165: 'hour', 166: 'soap', 167: 'sauce', 168: 'tuesday', 169: 'champion', 170: 'hat', 171: 'war', 172: 'sugar', 173: 'hi', 174: 'umbrella', 175: 'you', 176: 'bill', 177: 'free', 178: 'voice', 179: 'love', 180: 'evil', 181: 'border', 182: 'you', 183: 'say', 184: 'promise', 185: 'milk', 186: 'okay', 187: 'comb', 188: 'date', 189: 'holiday', 190: 'sweet', 191: 'ceiling', 192: 'danger', 193: 'telephone', 194: 'scales', 195: 'tailor', 196: 'thanks', 197: 'screwdriver', 198: 'turkey', 199: 'orange', 200: 'toilet', 201: 'flour', 202: 'far', 203: 'sad', 204: 'existing', 205: 'tax', 206: 'near', 207: 'alone', 208: 'wrong', 209: 'do', 210: 'band-aid', 211: 'help', 212: 'tomorrow', 213: 'forbidden', 214: 'pillow', 215: 'bed', 216: 'slow', 217: 'eat', 218: 'cook', 219: 'star', 220: 'absent', 221: 'road', 222: 'tired', 223: 'egg', 224: 'time', 225: 'difficult'}\n"
     ]
    }
   ],
   "source": [
    "print(class_id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5af8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(vid_path, frames_cap, transforms=None):\n",
    "    \"\"\"Extract and transform video frames\n",
    "\n",
    "    Parameters:\n",
    "    vid_path (str): path to video file\n",
    "    frames_cap (int): number of frames to extract, evenly spaced\n",
    "    transforms (torchvision.transforms, optional): transformations to apply to frame\n",
    "\n",
    "    Returns:\n",
    "    list of numpy.array: vid_arr\n",
    "\n",
    "    \"\"\"\n",
    "    vid_arr = []\n",
    "    with av.open(vid_path) as container:\n",
    "        stream = container.streams.video[0]\n",
    "        n_frames = stream.frames\n",
    "        remainder = n_frames % frames_cap\n",
    "        interval = n_frames // frames_cap\n",
    "        take_frame_idx = 0\n",
    "        for frame_no, frame in enumerate(container.decode(stream)):\n",
    "            if frame_no == take_frame_idx:\n",
    "                img = frame.to_image()\n",
    "                if transforms:\n",
    "                    img = transforms(img)\n",
    "                vid_arr.append(np.array(img))\n",
    "                if remainder > 0:\n",
    "                    take_frame_idx += 1\n",
    "                    remainder -= 1\n",
    "                take_frame_idx += interval\n",
    "    if len(vid_arr) < frames_cap:\n",
    "        raise ValueError(f\"video with path '{vid_path}' is too short, please make sure that video has >={frames_cap} frames\")\n",
    "    return vid_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c41124",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a00e70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819663b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_LSTM(\n",
       "  (encoder): CNN_Encoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (layers): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (a_fn): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): ConvBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (batchnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (a_fn): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): ConvBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (batchnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (a_fn): ReLU()\n",
       "      )\n",
       "      (3): ConvBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (batchnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (a_fn): ReLU()\n",
       "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (4): ConvBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (batchnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (a_fn): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv2): ConvBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (batchnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (batchnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (a_fn): ReLU()\n",
       "      (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): LSTM_Decoder(\n",
       "    (lstm): LSTM(512, 512, batch_first=True, bidirectional=True)\n",
       "    (fc1): Linear(in_features=1024, out_features=226, bias=True)\n",
       "    (a_fn): ReLU()\n",
       "    (attention_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.8, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_LSTM(226, \n",
    "                 latent_size=512, \n",
    "                 n_cnn_layers=6, \n",
    "                 n_rnn_layers=1, \n",
    "                 n_rnn_hidden_dim=512, \n",
    "                 cnn_bn=True, \n",
    "                 bidirectional=True, \n",
    "                 dropout_rate=0.8,\n",
    "                 device=\"cpu\",\n",
    "                 attention=True)\n",
    "checkpoint = torch.load(\"saved_models/train_final/final.pt\", map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11989550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = r2plus1d_18(pretrained=True, num_classes=226)\n",
    "#checkpoint = torch.load(\"saved_models/cnn_lstm_512_6_1_512_drop80_weightd8/18-checkpoint.pt\")\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf826f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(vid, transforms = None, frames_cap = 30):\n",
    "    \n",
    "    selector = fix_frame(len(vid), frames_cap)\n",
    "    output = []\n",
    "    for e,frame in enumerate(vid):\n",
    "        if e+1 in selector:\n",
    "            output.append(frame)\n",
    "    \n",
    "    # edge case\n",
    "    if len(vid) < frames_cap:\n",
    "        remainder = frames_cap - len(vid)\n",
    "        # take last frame\n",
    "        last_frame = vid[-1]\n",
    "        for _ in range(remainder):\n",
    "            output.append(last_frame)\n",
    "        \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d47812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking(rbg_vid, depth_vid):\n",
    "    \"\"\"\n",
    "    input\n",
    "        - path for rbg\n",
    "        - path for depth\n",
    "    output\n",
    "        - array of numpy arrays\n",
    "    \"\"\"\n",
    "    rbg_arr = []\n",
    "    container_rbg = av.open(rbg_vid)\n",
    "\n",
    "    for packet in container_rbg.demux():\n",
    "        for frame in packet.decode():\n",
    "            rbg_arr.append(np.array(frame.to_image()))\n",
    "\n",
    "    depth_arr = []\n",
    "    container_depth = av.open(depth_vid)\n",
    "\n",
    "    for packet in container_depth.demux():\n",
    "        for frame in packet.decode():\n",
    "            depth_arr.append(np.array(frame.to_image()))\n",
    "            \n",
    "    # pose estimation\n",
    "    #rbg_arr = pose_styling(rbg_arr)\n",
    "\n",
    "    # display - correct color orientation\n",
    "    overlay_arr = []\n",
    "    for i in range(len(rbg_arr)):\n",
    "        c = cv2.cvtColor(rbg_arr[i], cv2.COLOR_BGR2RGB)\n",
    "        gray = cv2.cvtColor(depth_arr[i], cv2.COLOR_BGR2GRAY)\n",
    "        overlay = cv2.bitwise_and(c,c, mask= gray)\n",
    "        \n",
    "        # resize and reshape\n",
    "        overlay = cv2.resize(overlay, (256,256))\n",
    "        \n",
    "        # convert from (h , w, c) to (c, h, w)\n",
    "        overlay_reshape = np.transpose(overlay, (2, 0, 1))\n",
    "        \n",
    "        overlay_arr.append(overlay_reshape)\n",
    "        \n",
    "    return np.array(overlay_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_frame(input_frame: int, output_frame: int) -> set:\n",
    "    '''\n",
    "    input\n",
    "        - number of input frames\n",
    "        - number of output frames\n",
    "    output\n",
    "        - a set of frames\n",
    "    '''\n",
    "    if input_frame < output_frame:\n",
    "        print('Spotted video that have input frame: {} < output frame: {}'.format(input_frame, output_frame))\n",
    "        return set([i for i in range(1, input_frame+1)])\n",
    "    \n",
    "    # create array to pick from\n",
    "    pick_arr = []\n",
    "    for i in range(1,input_frame+1):\n",
    "        for r in range(output_frame):\n",
    "            pick_arr.append(i)\n",
    "            \n",
    "    # decide on index to capture\n",
    "    # e.g. frame 58//2 = 29\n",
    "    ind = input_frame//2\n",
    "    \n",
    "    # capture frame\n",
    "    output = set()\n",
    "    i = 1\n",
    "    batch = 0\n",
    "    while (i + (batch * input_frame)) < len(pick_arr):\n",
    "        if i == ind:\n",
    "            output.add(pick_arr[i + (batch * input_frame) - 1])\n",
    "        i+=1\n",
    "        if i == input_frame + 1:\n",
    "            i = 1\n",
    "            batch += 1\n",
    "    if len(output) != output_frame:\n",
    "        raise ValueError('output does not have the same frame requirements. output: {}, required: {}'.format(len(output), output_frame))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6defd5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_video(vid_name):\n",
    "    \"\"\"Load video from dataset and pass video to model\n",
    "\n",
    "    Parameters:\n",
    "    vid_name (str): video name to display\n",
    "\n",
    "    \"\"\"\n",
    "    transforms_compose = transforms.Compose([transforms.Resize(256), \n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "    vid_color_path = f\"{test_dir}/{vid_name}_color.mp4\"\n",
    "    vid_depth_path = f\"{test_dir}/{vid_name}_depth.mp4\"\n",
    "    rgb_arr = extract_frames(vid_color_path, 30, transforms=transforms_compose)\n",
    "    vid_arr = np.array(rgb_arr)\n",
    "    vid_arr = vid_arr/255\n",
    "    #vid_arr = masking(vid_color_path, vid_depth_path)\n",
    "    #vid_arr = extract_frames(vid_arr, 30)\n",
    "    vid_arr = torch.from_numpy(vid_arr).float()\n",
    "    #vid_arr = vid_arr.permute(1, 0, 2, 3)\n",
    "    vid_arr = vid_arr.unsqueeze(0)\n",
    "    predict_id = model.forward(vid_arr)\n",
    "    predict_id = torch.max(predict_id, 1)[1].item()\n",
    "    ground_truth_id = test_label[f\"{test_dir}/{vid_name}\"]\n",
    "    return predict_id, class_id_to_label[predict_id], ground_truth_id, class_id_to_label[ground_truth_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bc77862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3739/3739 [52:01<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the test set and perform predictions on each sample. Add their ground truths and predictions to lists\n",
    "df = pd.read_csv(\"dataset/test_labels.csv\", names=[\"video_name\", \"class_id\"])\n",
    "predict_ids = []\n",
    "predict_labels = []\n",
    "ground_truth_ids = []\n",
    "ground_truth_labels = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    predict_id, predict_label, ground_truth_id, ground_truth_label = load_and_test_video(row['video_name'])\n",
    "    predict_ids.append(predict_id)\n",
    "    predict_labels.append(predict_label)\n",
    "    ground_truth_ids.append(ground_truth_id)\n",
    "    ground_truth_labels.append(ground_truth_label)\n",
    "\n",
    "# Load the test set and perform predictions on each sample. Add their ground truths and predictions to lists\n",
    "\n",
    "df['predict_id'] = predict_ids\n",
    "df['predict_label'] = predict_labels\n",
    "df['ground_truth_id'] = ground_truth_ids\n",
    "df['ground_truth_label'] = ground_truth_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ae1a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predict_id'] = predict_ids\n",
    "df['predict_label'] = predict_labels\n",
    "df['ground_truth_id'] = ground_truth_ids\n",
    "df['ground_truth_label'] = ground_truth_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69cb22da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>class_id</th>\n",
       "      <th>predict_id</th>\n",
       "      <th>predict_label</th>\n",
       "      <th>ground_truth_id</th>\n",
       "      <th>ground_truth_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>signer34_sample1</td>\n",
       "      <td>133</td>\n",
       "      <td>175</td>\n",
       "      <td>you</td>\n",
       "      <td>133</td>\n",
       "      <td>guest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>signer34_sample2</td>\n",
       "      <td>61</td>\n",
       "      <td>164</td>\n",
       "      <td>request</td>\n",
       "      <td>61</td>\n",
       "      <td>retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>signer34_sample3</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>hammer</td>\n",
       "      <td>32</td>\n",
       "      <td>together</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>signer34_sample4</td>\n",
       "      <td>169</td>\n",
       "      <td>39</td>\n",
       "      <td>teapot</td>\n",
       "      <td>169</td>\n",
       "      <td>champion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>signer34_sample5</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>show</td>\n",
       "      <td>77</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>signer30_sample658</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>salary</td>\n",
       "      <td>125</td>\n",
       "      <td>salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>signer30_sample659</td>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "      <td>ceiling</td>\n",
       "      <td>191</td>\n",
       "      <td>ceiling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>signer30_sample660</td>\n",
       "      <td>96</td>\n",
       "      <td>220</td>\n",
       "      <td>absent</td>\n",
       "      <td>96</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>signer30_sample661</td>\n",
       "      <td>59</td>\n",
       "      <td>142</td>\n",
       "      <td>wood</td>\n",
       "      <td>59</td>\n",
       "      <td>glove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>signer30_sample662</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>full</td>\n",
       "      <td>63</td>\n",
       "      <td>meal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3739 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              video_name  class_id  predict_id predict_label  ground_truth_id  \\\n",
       "0       signer34_sample1       133         175           you              133   \n",
       "1       signer34_sample2        61         164       request               61   \n",
       "2       signer34_sample3        32          40        hammer               32   \n",
       "3       signer34_sample4       169          39        teapot              169   \n",
       "4       signer34_sample5        77          77          show               77   \n",
       "...                  ...       ...         ...           ...              ...   \n",
       "3734  signer30_sample658       125         125        salary              125   \n",
       "3735  signer30_sample659       191         191       ceiling              191   \n",
       "3736  signer30_sample660        96         220        absent               96   \n",
       "3737  signer30_sample661        59         142          wood               59   \n",
       "3738  signer30_sample662        63          53          full               63   \n",
       "\n",
       "     ground_truth_label  \n",
       "0                 guest  \n",
       "1               retired  \n",
       "2              together  \n",
       "3              champion  \n",
       "4                  show  \n",
       "...                 ...  \n",
       "3734             salary  \n",
       "3735            ceiling  \n",
       "3736           medicine  \n",
       "3737              glove  \n",
       "3738               meal  \n",
       "\n",
       "[3739 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6a9564",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_id_to_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m target_names \u001b[38;5;241m=\u001b[39m [class_id_to_label[key] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m class_id_to_label\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m226\u001b[39m)]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(ground_truth_ids, predict_ids, target_names\u001b[38;5;241m=\u001b[39mtarget_names))\n\u001b[0;32m      3\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(ground_truth_ids, predict_ids,target_names\u001b[38;5;241m=\u001b[39mtarget_names, output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m target_names \u001b[38;5;241m=\u001b[39m [class_id_to_label[key] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclass_id_to_label\u001b[49m\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m226\u001b[39m)]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(ground_truth_ids, predict_ids, target_names\u001b[38;5;241m=\u001b[39mtarget_names))\n\u001b[0;32m      3\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(ground_truth_ids, predict_ids,target_names\u001b[38;5;241m=\u001b[39mtarget_names, output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_id_to_label' is not defined"
     ]
    }
   ],
   "source": [
    "target_names = [class_id_to_label[key] if key in class_id_to_label.keys() else 0 for key in range(226)]\n",
    "print(classification_report(ground_truth_ids, predict_ids, target_names=target_names))\n",
    "report = classification_report(ground_truth_ids, predict_ids,target_names=target_names, output_dict=True)\n",
    "df_classification_report = pd.DataFrame(report).transpose()\n",
    "df_classification_report = df_classification_report.sort_values(by=['precision'], ascending=False)\n",
    "df_classification_report.to_csv('normal_report.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total test data:{len(df)}')\n",
    "cm = confusion_matrix(ground_truth_ids, predict_ids)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.array(target_names))\n",
    "fig, ax = plt.subplots(figsize=(14,14))\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('masked_CNN_prediction.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764b7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
